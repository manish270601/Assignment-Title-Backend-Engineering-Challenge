{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTPmCnJYp3XC5dhw4e5AKm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manish270601/Assignment-Title-Backend-Engineering-Challenge/blob/main/Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q23739oUUcw",
        "outputId": "3c990236-03c4-4040-eb2b-6079d9d6542d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "data_scraper.py"
      ],
      "metadata": {
        "id": "uEGgURPlYHn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_data(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    project_titles = soup.find_all('h2', class_='project-title')\n",
        "    project_descriptions = soup.find_all('p', class_='project-description')\n",
        "    data = []\n",
        "    for title, description in zip(project_titles, project_descriptions):\n",
        "        data.append({\n",
        "            'title': title.text.strip(),\n",
        "            'description': description.text.strip()\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def main():\n",
        "    url = \"https://www.example.com/construction-projects\"\n",
        "    data = scrape_data(url)\n",
        "    data.to_csv('construction_projects.csv', index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ii2cRNyjUeYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_standardizer.py"
      ],
      "metadata": {
        "id": "67iEN8LlYO_5"
      }
    },
    {
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_data(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    project_titles = soup.find_all('h2', class_='project-title')\n",
        "    project_descriptions = soup.find_all('p', class_='project-description')\n",
        "\n",
        "    # Check if we found any data\n",
        "    if not project_titles or not project_descriptions:\n",
        "        print(\"Warning: No project titles or descriptions found. Check the website and selectors.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data is found\n",
        "\n",
        "    data = []\n",
        "    for title, description in zip(project_titles, project_descriptions):\n",
        "        data.append({\n",
        "            'title': title.text.strip(),\n",
        "            'description': description.text.strip()\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def main():\n",
        "    url = \"https://www.example.com/construction-projects\"\n",
        "    data = scrape_data(url)\n",
        "\n",
        "    # Check if the DataFrame is empty before saving\n",
        "    if data.empty:\n",
        "        print(\"Error: No data to save. Exiting.\")\n",
        "        return\n",
        "\n",
        "    data.to_csv('construction_projects.csv', index=False)\n",
        "    print(\"Data saved to construction_projects.csv\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhbczMsDVY5P",
        "outputId": "53a97d67-5e01-4f39-f826-de8dba91de08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No project titles or descriptions found. Check the website and selectors.\n",
            "Error: No data to save. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "language_model.py"
      ],
      "metadata": {
        "id": "a2vEr9C7YTqc"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def classify_text(text):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "    logits = outputs.last_hidden_state[:, 0, :]\n",
        "    return logits.detach().numpy()\n",
        "\n",
        "def main():\n",
        "    # Verify the file path\n",
        "    file_path = 'standardized_construction_projects.csv'\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        data['classification'] = data['description'].apply(classify_text)\n",
        "        data.to_csv('classified_construction_projects.csv', index=False)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found. Please check the file path.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUuD5YXoV7LH",
        "outputId": "65e0ca48-88f3-4051-f1a3-537ca268f2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File 'standardized_construction_projects.csv' not found. Please check the file path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "construction_projects.csv"
      ],
      "metadata": {
        "id": "0UsDuuYbYcjJ"
      }
    },
    {
      "source": [
        "title = \"Construction of a new building\"\n",
        "description = \"The construction of a new building is underway. The building will have 10 floors and will be used for office space.\"\n",
        "\n",
        "print(title)\n",
        "print(description)\n",
        "\n",
        "title = \"Infrastructure project\"\n",
        "description = \"The infrastructure project involves the construction of a new road. The road will be 10 km long and will connect two major cities.\"\n",
        "\n",
        "print(title)\n",
        "print(description)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIo9wPgCWIkt",
        "outputId": "1a007f1f-ff14-46db-e777-2e139254b608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construction of a new building\n",
            "The construction of a new building is underway. The building will have 10 floors and will be used for office space.\n",
            "Infrastructure project\n",
            "The infrastructure project involves the construction of a new road. The road will be 10 km long and will connect two major cities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "standardized_construction_projects.csv"
      ],
      "metadata": {
        "id": "3P93hPMfYd5Z"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    {\"title\": \"Construction of a New Building\", \"description\": \"The construction of a new building is underway. The building will have 10 floors and will be used for office space.\", \"sector\": \"Construction\", \"subsector\": \"Building\"},\n",
        "    {\"title\": \"Infrastructure Project\", \"description\": \"The infrastructure project involves the construction of a new road. The road will be 10 km long and will connect two major cities.\", \"sector\": \"Infrastructure\", \"subsector\": \"Road\"}\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODpBiEz_WSC8",
        "outputId": "4b3d8903-8385-4f89-f270-82bafe3c700f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            title  \\\n",
            "0  Construction of a New Building   \n",
            "1          Infrastructure Project   \n",
            "\n",
            "                                         description          sector subsector  \n",
            "0  The construction of a new building is underway...    Construction  Building  \n",
            "1  The infrastructure project involves the constr...  Infrastructure      Road  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classified_construction_projects.csv"
      ],
      "metadata": {
        "id": "9DWMk8mmYhcw"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    {\"title\": \"Construction of a New Building\",\n",
        "     \"description\": \"The construction of a new building is underway. The building will have 10 floors and will be used for office space.\",\n",
        "     \"sector\": \"Construction\",\n",
        "     \"subsector\": \"Building\",\n",
        "     \"classification\": 0.8},\n",
        "    {\"title\": \"Infrastructure Project\",\n",
        "     \"description\": \"The infrastructure project involves the construction of a new road. The road will be 10 km long and will connect two major cities.\",\n",
        "     \"sector\": \"Infrastructure\",\n",
        "     \"subsector\": \"Road\",\n",
        "     \"classification\": 0.4}\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqwId1EdYARK",
        "outputId": "e299fc6c-0f68-4838-b986-587c7f763bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            title  \\\n",
            "0  Construction of a New Building   \n",
            "1          Infrastructure Project   \n",
            "\n",
            "                                         description          sector  \\\n",
            "0  The construction of a new building is underway...    Construction   \n",
            "1  The infrastructure project involves the constr...  Infrastructure   \n",
            "\n",
            "  subsector  classification  \n",
            "0  Building             0.8  \n",
            "1      Road             0.4  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. California Department of Transportation (Caltrans)\n",
        "\n"
      ],
      "metadata": {
        "id": "GKpnD-WCdDYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_and_standardize_data(url, table_data):\n",
        "    \"\"\"\n",
        "    Scrapes data from a website and standardizes it based on a provided table.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the website to scrape data from.\n",
        "        table_data (list): A list of dictionaries, where each dictionary represents a row in the standardization table.\n",
        "            Each dictionary should have the following keys:\n",
        "                - 'source_name': The name of the field in the scraped data.\n",
        "                - 'standard_name': The standardized name for the field.\n",
        "                - 'conversion' (optional): A function to convert the scraped data to the standardized format.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the scraped and standardized data.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find project elements (modify selectors based on website structure)\n",
        "    project_elements = soup.find_all('div', class_='project-card')  # Example\n",
        "\n",
        "    data = []\n",
        "    for project in project_elements:\n",
        "        project_dict = {}\n",
        "        for table_field in table_data:\n",
        "            source_name = table_field['source_name']\n",
        "            # Extract data using the source field name\n",
        "            project_dict[source_name] = project.find(selector=table_field.get('selector', None)).text.strip()\n",
        "\n",
        "        data.append(project_dict)\n",
        "\n",
        "    # Standardize data\n",
        "    standardized_data = []\n",
        "    for project in data:\n",
        "        standardized_project = {}\n",
        "        for table_field in table_data:\n",
        "            source_name = table_field['source_name']\n",
        "            standard_name = table_field['standard_name']\n",
        "            value = project.get(source_name)\n",
        "\n",
        "            # Apply conversion function if provided\n",
        "            conversion = table_field.get('conversion', None)\n",
        "            if conversion:\n",
        "                value = conversion(value)\n",
        "\n",
        "            standardized_project[standard_name] = value\n",
        "\n",
        "        standardized_data.append(standardized_project)\n",
        "\n",
        "    return pd.DataFrame(standardized_data)\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Define standardization table based on your table (replace with actual values)\n",
        "standardization_table = [\n",
        "    {\n",
        "        'source_name': 'h3',  # Replace with the selector for project title\n",
        "        'standard_name': 'title'\n",
        "    },\n",
        "    {\n",
        "        'source_name': 'p',  # Replace with the selector for project location\n",
        "        'standard_name': 'location'\n",
        "    },\n",
        "    # ... add entries for other fields ...\n",
        "    {\n",
        "        'source_name': 'p',  # Replace with the selector for project description (if available)\n",
        "        'standard_name': 'description'\n",
        "    },\n",
        "]\n",
        "\n",
        "# Replace with the actual URL for scraping projects\n",
        "url = 'https://dot.ca.gov/projects'\n",
        "\n",
        "# Scrape"
      ],
      "metadata": {
        "id": "v6eYIW4zdF8X"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. California State Controller's Office\n"
      ],
      "metadata": {
        "id": "-6JA0WbXWYoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_and_standardize_data(url, table_data):\n",
        "    \"\"\"\n",
        "    Scrapes data from a website and standardizes it based on a provided table.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the website to scrape data from.\n",
        "        table_data (list): A list of dictionaries, where each dictionary represents a row in the standardization table.\n",
        "            Each dictionary should have the following keys:\n",
        "                - 'source_name': The name of the field in the scraped data.\n",
        "                - 'standard_name': The standardized name for the field.\n",
        "                - 'conversion' (optional): A function to convert the scraped data to the standardized format.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the scraped and standardized data.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find contract elements (modify selectors based on website structure)\n",
        "    contract_elements = soup.find_all('tr', class_='contract-row')  # Example\n",
        "\n",
        "    data = []\n",
        "    for contract in contract_elements:\n",
        "        project_dict = {}\n",
        "        for table_field in table_data:\n",
        "            source_name = table_field['source_name']\n",
        "            # Extract data using the source field name\n",
        "            project_dict[source_name] = contract.find(selector=table_field.get('selector', None)).text.strip()\n",
        "\n",
        "        data.append(project_dict)\n",
        "\n",
        "    # Standardize data\n",
        "    standardized_data = []\n",
        "    for project in data:\n",
        "        standardized_project = {}\n",
        "        for table_field in table_data:\n",
        "            source_name = table_field['source_name']\n",
        "            standard_name = table_field['standard_name']\n",
        "            value = project.get(source_name)\n",
        "\n",
        "            # Apply conversion function if provided\n",
        "            conversion = table_field.get('conversion', None)\n",
        "            if conversion:\n",
        "                value = conversion(value)\n",
        "\n",
        "            standardized_project[standard_name] = value\n",
        "\n",
        "        standardized_data.append(standardized_project)\n",
        "\n",
        "    return pd.DataFrame(standardized_data)\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Define standardization table based on your table (replace with actual values)\n",
        "standardization_table = [\n",
        "    {\n",
        "        'source_name': 'td', 'selector': 'td:contains(\"Contract ID\")',  # Target cell containing \"Contract ID\"\n",
        "        'standard_name': 'Contract ID'\n",
        "    },\n",
        "    {\n",
        "        'source_name': 'td', 'selector': 'td:contains(\"Awarded Amount\")',  # Target cell containing \"Awarded Amount\"\n",
        "        'standard_name': 'Awarded Amount',\n",
        "        'conversion': lambda x: float(x.replace('$', '').replace(',', ''))  # Convert to numerical value\n",
        "    },\n",
        "    # ... add entries for other fields ...\n",
        "    {\n",
        "        'source_name': 'a', 'selector': 'td.vendor-name a',  # Assuming vendor info is in a link within \"vendor-name\" class\n",
        "        'standard_name': 'Vendor Information',\n",
        "        'conversion': lambda x: x.text.strip()  # Extract text from link\n",
        "    },\n",
        "]\n",
        "\n",
        "# Replace with the actual URL for scraping contracts\n",
        "url = 'https://sco.ca.gov/procurement/contracts'\n",
        "\n",
        "# Scrape and standardize data\n",
        "df = scrape_and_standardize_data(url, standardization_table)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('controller_contracts_standardized.csv', index=False)"
      ],
      "metadata": {
        "id": "gv9V_JSadNY6"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. City of Los Angeles Department of Public Works"
      ],
      "metadata": {
        "id": "PMgSNGYgdUyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_and_standardize_data(url, table_data):\n",
        "    \"\"\"\n",
        "    Scrapes data from a website and standardizes it based on a provided table.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the website to scrape data from.\n",
        "        table_data (list): A list of dictionaries, where each dictionary represents a row in the standardization table.\n",
        "            Each dictionary should have the following keys:\n",
        "                - 'source_name': The name of the field in the scraped data.\n",
        "                - 'standard_name': The standardized name for the field.\n",
        "                - 'conversion' (optional): A function to convert the scraped data to the standardized format.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing the scraped and standardized data.\n",
        "    \"\"\"\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find project elements (modify selectors based on website structure)\n",
        "    project_elements = soup.find_all('div', class_='project-card')  # Example\n",
        "\n",
        "    data = []\n",
        "    for project in project_elements:\n",
        "        project_dict = {}\n",
        "        for source_field in table_data:\n",
        "            source_name = source_field['source_name']\n",
        "            # Extract data using the source field name\n",
        "            project_dict[source_name] = project.find(selector=source_field.get('selector', None)).text.strip()\n",
        "\n",
        "        data.append(project_dict)\n",
        "\n",
        "    # Standardize data\n",
        "    standardized_data = []\n",
        "    for project in data:\n",
        "        standardized_project = {}\n",
        "        for table_field in table_data:\n",
        "            source_name = table_field['source_name']\n",
        "            standard_name = table_field['standard_name']\n",
        "            value = project.get(source_name)\n",
        "\n",
        "            # Apply conversion function if provided\n",
        "            conversion = table_field.get('conversion', None)\n",
        "            if conversion:\n",
        "                value = conversion(value)\n",
        "\n",
        "            standardized_project[standard_name] = value\n",
        "\n",
        "        standardized_data.append(standardized_project)\n",
        "\n",
        "    return pd.DataFrame(standardized_data)\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Define standardization table based on your table (replace with actual values)\n",
        "standardization_table = [\n",
        "    {\n",
        "        'source_name': 'h3',  # Replace with the selector for project title\n",
        "        'standard_name': 'Project Title'\n",
        "    },\n",
        "    {\n",
        "        'source_name': 'p',  # Replace with the selector for project description\n",
        "        'standard_name': 'Description',\n",
        "    },\n",
        "    # ... add entries for other fields ...\n",
        "    {\n",
        "        'source_name': 'location',  # Replace with the selector for project location (if available)\n",
        "        'standard_name': 'Location',\n",
        "        'conversion': lambda x: x.split(',')[0].strip()  # Example conversion for location (optional)\n",
        "    },\n",
        "]\n",
        "\n",
        "# Replace with the actual URL for scraping projects\n",
        "url = 'https://dpw.lacity.org/projects'\n",
        "\n",
        "# Scrape and standardize data\n",
        "df = scrape_and_standardize_data(url, standardization_table)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('la_dpw_projects_standardized.csv', index=False)"
      ],
      "metadata": {
        "id": "8GbBzZE3dPsq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. City of San Francisco Public Works"
      ],
      "metadata": {
        "id": "YO9Onqd-dZdA"
      }
    },
    {
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_sf_public_works(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find project elements (modify selectors based on website structure)\n",
        "    project_elements = soup.find_all('div', class_='project-card')  # Example, adjust selector if needed\n",
        "\n",
        "    data = []\n",
        "    for project in project_elements:  # Start of the loop\n",
        "        project_id = project.find('span', class_='project-id').text.strip()\n",
        "        title = project.find('h3').text.strip()\n",
        "        location = project.find('p', class_='location').text.strip()\n",
        "        # ... extract other fields like start date, end date, budget, status, updates ...\n",
        "        data.append({\n",
        "            'Project ID': project_id,\n",
        "            'Title': title,\n",
        "            'Location': location,\n",
        "            'Start Date': '',  # Extract from project details page\n",
        "            'End Date': '',  # Extract from project details page\n",
        "            'Budget': '',  # Extract from project details page\n",
        "            'Status': '',  # Extract from project details page\n",
        "            'Project Updates': '',  # Extract from project details page\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example usage:\n",
        "sf_public_works_url = 'https://sfpublicworks.org/projects'  # Replace with actual URL\n",
        "sf_public_works_data = scrape_sf_public_works(sf_public_works_url)\n",
        "\n",
        "# Apply standardization\n",
        "standardized_data = standardize_data(sf_public_works_data)\n",
        "\n",
        "df = pd.DataFrame(standardized_data)\n",
        "df.to_csv('sf_public_works_projects_standardized.csv', index=False)\n",
        "\n",
        "def standardize_data(data):\n",
        "  \"\"\"\n",
        "  Standardizes scraped data by mapping attribute names to desired names based on a table.\n",
        "\n",
        "  Args:\n",
        "      data: A list of dictionaries containing scraped project information.\n",
        "\n",
        "  Returns:\n",
        "      A list of dictionaries with standardized attribute names.\n",
        "  \"\"\"\n",
        "  # Replace with the actual table mapping (refer to the table you sent)\n",
        "  standardization_map = {\n",
        "      'Project ID': 'original_id',\n",
        "      'Title': 'title',\n",
        "      'Location': 'location_name',\n",
        "      # ... add mappings for other attributes ...\n",
        "  }\n",
        "\n",
        "  standardized_data = []\n",
        "  for project in data:\n",
        "    new_project = {}\n",
        "    for old_attr, value in project.items():\n",
        "      new_attr = standardization_map.get(old_attr, old_attr)  # Use original attribute name if not found in mapping\n",
        "      new_project[new_attr] = value\n",
        "    standardized_data.append(new_project)\n",
        "  return standardized_data"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Cxt4eEJ2eEKm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. City of San Diego Public Works"
      ],
      "metadata": {
        "id": "C1vsoZYUeIYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_and_standardize(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find project elements (modify selectors based on website structure)\n",
        "    project_elements = soup.find_all('div', class_='project-card')  # Example selector\n",
        "\n",
        "    data = []\n",
        "    for project in project_elements:\n",
        "        project_name = project.find('h3').text.strip()\n",
        "        description = project.find('p', class_='description').text.strip()\n",
        "        # ... extract other fields like location, estimated cost, timeline, status ...\n",
        "\n",
        "        # Assuming a table with headers matching your data fields\n",
        "        table_data = project.find('table')  # Modify selector if the table is within a different element\n",
        "        if table_data:\n",
        "            table_rows = table_data.find_all('tr')\n",
        "            for row in table_rows:\n",
        "                # Extract data based on header positions (assuming headers are in the first row)\n",
        "                data_cells = row.find_all('td')\n",
        "                if len(data_cells) > 1:  # Skip header row if it exists\n",
        "                    standardized_location = standardize_location(data_cells[0].text.strip())  # Replace index based on your table structure\n",
        "                    standardized_cost = standardize_cost(data_cells[1].text.strip())  # Replace index based on your table structure\n",
        "                    # ... standardize other fields based on their positions ...\n",
        "                    data.append({\n",
        "                        'Project Name': project_name,\n",
        "                        'Description': description,\n",
        "                        'Location': standardized_location,\n",
        "                        'Estimated Cost': standardized_cost,\n",
        "                        'Timeline': '',  # Extract from project details page (if available)\n",
        "                        'Status': '',  # Extract from project details page (if available)\n",
        "                    })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Example standardization functions (replace with your specific logic)\n",
        "def standardize_location(location_string):\n",
        "    # Your logic to standardize location data (e.g., remove unnecessary characters, convert to uppercase/lowercase)\n",
        "    return location_string.upper()\n",
        "\n",
        "def standardize_cost(cost_string):\n",
        "    # Your logic to standardize cost data (e.g., remove currency symbols, convert to numeric format)\n",
        "    # You might need libraries like `re` for regular expressions\n",
        "    return cost_string.replace(\"$\", \"\")\n",
        "\n",
        "# Example usage\n",
        "sd_public_works_url = 'https://www.sandiego.gov/public-works/projects'  # Replace with actual URL\n",
        "sd_public_works_data = scrape_and_standardize(sd_public_works_url)\n",
        "df = pd.DataFrame(sd_public_works_data)\n",
        "df.to_csv('sd_public_works_projects.csv', index=False)"
      ],
      "metadata": {
        "id": "fWYhmCX5dbmq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.California Energy Commission"
      ],
      "metadata": {
        "id": "bd1yZq5yiXQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_energy_commission(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find project elements (modify selectors based on website structure)\n",
        "    project_elements = soup.find_all('li', class_='program-item')  # Example selector\n",
        "\n",
        "    data = []\n",
        "    for project in project_elements:\n",
        "        # Extract project details based on HTML structure\n",
        "        project_title = project.find('a', class_='program-title').text.strip()\n",
        "        project_link = project.find('a', class_='program-title')['href']  # Get link for details\n",
        "\n",
        "        # ... extract other fields like project ID, funding amount, location, etc. ...\n",
        "\n",
        "        # Might need to follow project link for details like funding, location, etc.\n",
        "\n",
        "        data.append({\n",
        "            'Project Title': project_title,\n",
        "            'Project Link': project_link,\n",
        "            'Project ID': '',  # Extract from project details page (if available)\n",
        "            'Funding Amount': '',  # Extract from project details page (if available)\n",
        "            'Project Location': '',  # Extract from project details page (if available)\n",
        "            # ... other fields ...\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "# Data standardization functions (modify based on actual data types)\n",
        "def standardize_funding_amount(amount_str):\n",
        "    # Remove all characters except numbers, \".\", \"+\" or \"-\".\n",
        "    amount_str = re.sub(r\"[^\\d\\-+\\.]\", \"\", amount_str)\n",
        "    # Convert to float and round to 2 decimal places (assuming currency).\n",
        "    return float(amount_str) if amount_str else None\n",
        "\n",
        "def standardize_location(location_str):\n",
        "    # Split by commas, capitalize the first letter of each word, and join back.\n",
        "    words = location_str.split(\",\")\n",
        "    words = [word.capitalize() for word in words]\n",
        "    return \", \".join(words)\n",
        "\n",
        "# Example usage\n",
        "url = 'https://www.energy.ca.gov/grants-and-funding/programs'  # Replace with specific program listing\n",
        "data = scrape_energy_commission(url)\n",
        "\n",
        "# Apply standardization functions to relevant columns\n",
        "for project in data:\n",
        "    project['Funding Amount'] = standardize_funding_amount(project['Funding Amount'])\n",
        "    project['Project Location'] = standardize_location(project['Project Location'])\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('energy_commission_projects.csv', index=False)"
      ],
      "metadata": {
        "id": "Rjl0RZgReLL5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsYNUFtdiYVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}